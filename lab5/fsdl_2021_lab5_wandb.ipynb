{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fsdl_2021_lab5_wandb.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epW5aWUnIKnj",
        "outputId": "e5076095-6939-4a21-9495-10c2c3b24c6b"
      },
      "source": [
        "# FSDL Spring 2021 Setup\n",
        "!git clone https://github.com/full-stack-deep-learning/fsdl-text-recognizer-2021-labs\n",
        "%cd fsdl-text-recognizer-2021-labs\n",
        "!pip3 install boltons wandb pytorch_lightning==1.1.4 pip install torch==1.7.1+cu110 torchvision==0.8.2+cu110 torchaudio==0.7.2 torchtext==0.8.1 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "%env PYTHONPATH=.:$PYTHONPATH"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'fsdl-text-recognizer-2021-labs'...\n",
            "remote: Enumerating objects: 798, done.\u001b[K\n",
            "remote: Counting objects: 100% (232/232), done.\u001b[K\n",
            "remote: Compressing objects: 100% (93/93), done.\u001b[K\n",
            "remote: Total 798 (delta 158), reused 140 (delta 139), pack-reused 566\u001b[K\n",
            "Receiving objects: 100% (798/798), 18.89 MiB | 10.11 MiB/s, done.\n",
            "Resolving deltas: 100% (394/394), done.\n",
            "/content/fsdl-text-recognizer-2021-labs\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting boltons\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/a7/1a31561d10a089fcb46fe286766dd4e053a12f6e23b4fd1c26478aff2475/boltons-21.0.0-py2.py3-none-any.whl (193kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 12.2MB/s \n",
            "\u001b[?25hCollecting wandb\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e0/b4/9d92953d8cddc8450c859be12e3dbdd4c7754fb8def94c28b3b351c6ee4e/wandb-0.10.32-py2.py3-none-any.whl (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 27.0MB/s \n",
            "\u001b[?25hCollecting pytorch_lightning==1.1.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/12/98/86a89dcd54f84582bbf24cb29cd104b966fcf934d92d5dfc626f225015d2/pytorch_lightning-1.1.4-py3-none-any.whl (684kB)\n",
            "\u001b[K     |████████████████████████████████| 686kB 35.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (19.3.1)\n",
            "Requirement already satisfied: install in /usr/local/lib/python3.7/dist-packages (1.3.4)\n",
            "Collecting torch==1.7.1+cu110\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu110/torch-1.7.1%2Bcu110-cp37-cp37m-linux_x86_64.whl (1156.8MB)\n",
            "\u001b[K     |███████████████████████         | 834.1MB 1.3MB/s eta 0:04:03tcmalloc: large alloc 1147494400 bytes == 0x5576ef69e000 @  0x7fb58009d615 0x5576b5242cdc 0x5576b532252a 0x5576b5245afd 0x5576b5336fed 0x5576b52b9988 0x5576b52b44ae 0x5576b52473ea 0x5576b52b97f0 0x5576b52b44ae 0x5576b52473ea 0x5576b52b632a 0x5576b5337e36 0x5576b52b5853 0x5576b5337e36 0x5576b52b5853 0x5576b5337e36 0x5576b52b5853 0x5576b5337e36 0x5576b53ba3e1 0x5576b531a6a9 0x5576b5285cc4 0x5576b5246559 0x5576b52ba4f8 0x5576b524730a 0x5576b52b53b5 0x5576b52b47ad 0x5576b52473ea 0x5576b52b53b5 0x5576b524730a 0x5576b52b53b5\n",
            "\u001b[K     |█████████████████████████████▏  | 1055.7MB 1.4MB/s eta 0:01:13tcmalloc: large alloc 1434370048 bytes == 0x557733cf4000 @  0x7fb58009d615 0x5576b5242cdc 0x5576b532252a 0x5576b5245afd 0x5576b5336fed 0x5576b52b9988 0x5576b52b44ae 0x5576b52473ea 0x5576b52b97f0 0x5576b52b44ae 0x5576b52473ea 0x5576b52b632a 0x5576b5337e36 0x5576b52b5853 0x5576b5337e36 0x5576b52b5853 0x5576b5337e36 0x5576b52b5853 0x5576b5337e36 0x5576b53ba3e1 0x5576b531a6a9 0x5576b5285cc4 0x5576b5246559 0x5576b52ba4f8 0x5576b524730a 0x5576b52b53b5 0x5576b52b47ad 0x5576b52473ea 0x5576b52b53b5 0x5576b524730a 0x5576b52b53b5\n",
            "\u001b[K     |████████████████████████████████| 1156.7MB 1.3MB/s eta 0:00:01tcmalloc: large alloc 1445945344 bytes == 0x5577894e0000 @  0x7fb58009d615 0x5576b5242cdc 0x5576b532252a 0x5576b5245afd 0x5576b5336fed 0x5576b52b9988 0x5576b52b44ae 0x5576b52473ea 0x5576b52b560e 0x5576b52b44ae 0x5576b52473ea 0x5576b52b560e 0x5576b52b44ae 0x5576b52473ea 0x5576b52b560e 0x5576b52b44ae 0x5576b52473ea 0x5576b52b560e 0x5576b52b44ae 0x5576b52473ea 0x5576b52b560e 0x5576b524730a 0x5576b52b560e 0x5576b52b44ae 0x5576b52473ea 0x5576b52b632a 0x5576b52b44ae 0x5576b52473ea 0x5576b52b632a 0x5576b52b44ae 0x5576b5247a81\n",
            "\u001b[K     |████████████████████████████████| 1156.8MB 16kB/s \n",
            "\u001b[?25hCollecting torchvision==0.8.2+cu110\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu110/torchvision-0.8.2%2Bcu110-cp37-cp37m-linux_x86_64.whl (12.9MB)\n",
            "\u001b[K     |████████████████████████████████| 12.9MB 256kB/s \n",
            "\u001b[?25hCollecting torchaudio==0.7.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/37/16/ecdb9eb09ec6b8133d6c9536ea9e49cd13c9b5873c8488b8b765a39028da/torchaudio-0.7.2-cp37-cp37m-manylinux1_x86_64.whl (7.6MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6MB 29.6MB/s \n",
            "\u001b[?25hCollecting torchtext==0.8.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/80/046f0691b296e755ae884df3ca98033cb9afcaf287603b2b7999e94640b8/torchtext-0.8.1-cp37-cp37m-manylinux1_x86_64.whl (7.0MB)\n",
            "\u001b[K     |████████████████████████████████| 7.0MB 19.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/fd/01/ff260a18caaf4457eb028c96eeb405c4a230ca06c8ec9c1379f813caa52e/configparser-5.0.2-py3-none-any.whl\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.12.4)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.1)\n",
            "Collecting subprocess32>=3.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 10.1MB/s \n",
            "\u001b[?25hCollecting shortuuid>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Collecting sentry-sdk>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/4a/a54b254f67d8f4052338d54ebe90126f200693440a93ef76d254d581e3ec/sentry_sdk-1.1.0-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 42.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Collecting pathtools\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Collecting GitPython>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/91/b38c4fabb6e5092ab23492ded4f318ab7299b19263272b703478038c0fbc/GitPython-3.1.18-py3-none-any.whl (170kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 40.8MB/s \n",
            "\u001b[?25hCollecting fsspec[http]>=0.8.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0e/3a/666e63625a19883ae8e1674099e631f9737bd5478c4790e5ad49c5ac5261/fsspec-2021.6.1-py3-none-any.whl (115kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 42.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning==1.1.4) (1.19.5)\n",
            "Collecting future>=0.17.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n",
            "\u001b[K     |████████████████████████████████| 829kB 35.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning==1.1.4) (2.5.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning==1.1.4) (4.41.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1+cu110) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.8.2+cu110) (7.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.0->wandb) (57.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/e8/f414d1a4f0bbc668ed441f74f44c116d9816833a48bf81d22b697090dba8/gitdb-4.0.7-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.4MB/s \n",
            "\u001b[?25hCollecting aiohttp; extra == \"http\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/c0/5890b4c8b04a79b7360e8fe4490feb0bb3ab179743f199f0e6220cebd568/aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 31.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.1.4) (0.4.4)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.1.4) (0.6.1)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.1.4) (0.36.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.1.4) (3.3.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.1.4) (1.0.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.1.4) (0.12.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.1.4) (1.34.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.1.4) (1.31.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.1.4) (1.8.0)\n",
            "Collecting smmap<5,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/68/ee/d540eb5e5996eb81c26ceffac6ee49041d473bc5125f2aa995cf51ec1cf1/smmap-4.0.0-py2.py3-none-any.whl\n",
            "Collecting yarl<2.0,>=1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f1/62/046834c5fc998c88ab2ef722f5d42122230a632212c8afa76418324f53ff/yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 40.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch_lightning==1.1.4) (21.2.0)\n",
            "Collecting async-timeout<4.0,>=3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e1/1e/5a4441be21b0726c4464f3f23c8b19628372f606755a9d2e46c187e65ec4/async_timeout-3.0.1-py3-none-any.whl\n",
            "Collecting multidict<7.0,>=4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/a6/4123b8165acbe773d1a8dc8e3f0d1edea16d29f7de018eda769abb56bd30/multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 39.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning==1.1.4) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning==1.1.4) (4.5.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning==1.1.4) (4.2.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning==1.1.4) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning==1.1.4) (0.2.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning==1.1.4) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning==1.1.4) (3.4.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning==1.1.4) (0.4.8)\n",
            "Building wheels for collected packages: subprocess32, pathtools, future\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp37-none-any.whl size=6502 sha256=1ef4ef9d5b9b7807edf0c5261524e3b52129424b23762262805a17fa254af484\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-cp37-none-any.whl size=8807 sha256=721ce1b5597645873446dbc5ce53a917aee3943a32ea9ca73f6f77f1c3956cf6\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-cp37-none-any.whl size=491070 sha256=55bfe78109751ce64c424fa8ae8e795f8ebd3607029c72681093f8f429d421f7\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n",
            "Successfully built subprocess32 pathtools future\n",
            "\u001b[31mERROR: pytorch-lightning 1.1.4 has requirement PyYAML>=5.1, but you'll have pyyaml 3.13 which is incompatible.\u001b[0m\n",
            "Installing collected packages: boltons, configparser, subprocess32, shortuuid, sentry-sdk, pathtools, docker-pycreds, smmap, gitdb, GitPython, wandb, torch, multidict, yarl, async-timeout, aiohttp, fsspec, future, pytorch-lightning, torchvision, torchaudio, torchtext\n",
            "  Found existing installation: torch 1.9.0+cu102\n",
            "    Uninstalling torch-1.9.0+cu102:\n",
            "      Successfully uninstalled torch-1.9.0+cu102\n",
            "  Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "  Found existing installation: torchvision 0.10.0+cu102\n",
            "    Uninstalling torchvision-0.10.0+cu102:\n",
            "      Successfully uninstalled torchvision-0.10.0+cu102\n",
            "  Found existing installation: torchtext 0.10.0\n",
            "    Uninstalling torchtext-0.10.0:\n",
            "      Successfully uninstalled torchtext-0.10.0\n",
            "Successfully installed GitPython-3.1.18 aiohttp-3.7.4.post0 async-timeout-3.0.1 boltons-21.0.0 configparser-5.0.2 docker-pycreds-0.4.0 fsspec-2021.6.1 future-0.18.2 gitdb-4.0.7 multidict-5.1.0 pathtools-0.1.2 pytorch-lightning-1.1.4 sentry-sdk-1.1.0 shortuuid-1.0.1 smmap-4.0.0 subprocess32-3.5.4 torch-1.7.1+cu110 torchaudio-0.7.2 torchtext-0.8.1 torchvision-0.8.2+cu110 wandb-0.10.32 yarl-1.6.3\n",
            "env: PYTHONPATH=.:$PYTHONPATH\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KgDQp33KTHG",
        "outputId": "d3d31d4a-013a-4395-9222-1f79cb577783"
      },
      "source": [
        "cd lab5"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/fsdl-text-recognizer-2021-labs/lab5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCXe62daMlxw",
        "outputId": "7e40dc50-db65-49f8-a809-ad04f7eccf7d"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/fsdl-text-recognizer-2021-labs/lab5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WJiKvhFKZE3",
        "outputId": "75bdc477-596e-4416-8108-0909a0645d24"
      },
      "source": [
        "!wandb init"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m\u001b[1mLet's setup this directory for W&B!\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Which project should we use?\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) fsdl-text-recognizer-2021-labs\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Create New\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: fsdl-text-recognizer-2021-labs\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Invalid choice\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'fsdl-text-recognizer-2021-labs'\n",
            "\u001b[32mThis directory is configured!  Next, track a run:\n",
            "\u001b[0m* In your training script:\n",
            "    \u001b[1mimport wandb\u001b[0m\n",
            "    \u001b[1mwandb.init(project=\"fsdl-text-recognizer-2021-labs\")\u001b[0m\n",
            "* then `\u001b[1mpython <train.py>\u001b[0m`.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDi8U2m9LBxI",
        "outputId": "4dff87fa-6b6e-4117-f6f6-f560b06d5b04"
      },
      "source": [
        "!python training/run_experiment.py --wandb --max_epochs=10 --gpus='0,' --num_workers=4 --data_class=EMNISTLines2 --model_class=LineCNNTransformer --loss=transformer"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlinxiao\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "2021-06-23 14:09:28.695621: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msweet-valley-3\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/linxiao/fsdl-text-recognizer-2021-labs\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/linxiao/fsdl-text-recognizer-2021-labs/runs/206iyf2o\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/fsdl-text-recognizer-2021-labs/lab5/wandb/run-20210623_140927-206iyf2o\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: You have set progress_bar_refresh_rate < 20 on Google Colab. This may crash. Consider using progress_bar_refresh_rate >= 20 in Trainer.\n",
            "  warnings.warn(*args, **kwargs)\n",
            "GPU available: True, used: True\n",
            "TPU available: None, using: 0 TPU cores\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "EMNISTLines2 generating data for train...\n",
            "[nltk_data] Downloading package brown to /content/fsdl-text-\n",
            "[nltk_data]     recognizer-2021-labs/data/downloaded/nltk...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "Downloading raw dataset from https://s3-us-west-2.amazonaws.com/fsdl-public-assets/matlab.zip to /content/fsdl-text-recognizer-2021-labs/data/downloaded/emnist/matlab.zip...\n",
            "709MB [00:41, 17.9MB/s]               \n",
            "Computing SHA-256...\n",
            "Unzipping EMNIST...\n",
            "Loading training data from .mat file\n",
            "Balancing classes to reduce amount of data\n",
            "Saving to HDF5 in a compressed format...\n",
            "Saving essential dataset parameters to text_recognizer/datasets...\n",
            "Cleaning up...\n",
            "tcmalloc: large alloc 2293760000 bytes == 0x564d39346000 @  0x7fe8db381b6b 0x7fe8db3a1379 0x7fe82d47774e 0x7fe82d4797b6 0x7fe8b89e7ed2 0x7fe8b8cd2b03 0x7fe8b8caa137 0x7fe8b8cc520c 0x7fe8b8ca16ba 0x7fe8b8caa137 0x7fe8b8cc520c 0x7fe8b8d9100d 0x7fe8b89de955 0x7fe8b8f04bc7 0x7fe8b8f53455 0x7fe8b85b50ce 0x7fe8b8cce623 0x7fe8b8ca7ed2 0x7fe8b85b50ce 0x7fe8b8cce623 0x7fe8b8db4b36 0x7fe8c8a74807 0x564ce51a1cc0 0x564ce51a1a50 0x564ce5215be0 0x564ce51a330a 0x564ce52113b5 0x564ce51a330a 0x564ce521160e 0x564ce52104ae 0x564ce50e2eb1\n",
            "EMNISTLines2 generating data for val...\n",
            "EMNISTLines2 generating data for test...\n",
            "EMNISTLines2 loading data from HDF5...\n",
            "\n",
            "    | Name                                                       | Type                    | Params\n",
            "---------------------------------------------------------------------------------------------------------\n",
            "0   | model                                                      | LineCNNTransformer      | 4.3 M \n",
            "1   | model.line_cnn                                             | LineCNN                 | 1.6 M \n",
            "2   | model.line_cnn.convs                                       | Sequential              | 1.2 M \n",
            "3   | model.line_cnn.convs.0                                     | ConvBlock               | 320   \n",
            "4   | model.line_cnn.convs.0.conv                                | Conv2d                  | 320   \n",
            "5   | model.line_cnn.convs.0.relu                                | ReLU                    | 0     \n",
            "6   | model.line_cnn.convs.1                                     | ConvBlock               | 9.2 K \n",
            "7   | model.line_cnn.convs.1.conv                                | Conv2d                  | 9.2 K \n",
            "8   | model.line_cnn.convs.1.relu                                | ReLU                    | 0     \n",
            "9   | model.line_cnn.convs.2                                     | ConvBlock               | 9.2 K \n",
            "10  | model.line_cnn.convs.2.conv                                | Conv2d                  | 9.2 K \n",
            "11  | model.line_cnn.convs.2.relu                                | ReLU                    | 0     \n",
            "12  | model.line_cnn.convs.3                                     | ConvBlock               | 9.2 K \n",
            "13  | model.line_cnn.convs.3.conv                                | Conv2d                  | 9.2 K \n",
            "14  | model.line_cnn.convs.3.relu                                | ReLU                    | 0     \n",
            "15  | model.line_cnn.convs.4                                     | ConvBlock               | 18.5 K\n",
            "16  | model.line_cnn.convs.4.conv                                | Conv2d                  | 18.5 K\n",
            "17  | model.line_cnn.convs.4.relu                                | ReLU                    | 0     \n",
            "18  | model.line_cnn.convs.5                                     | ConvBlock               | 36.9 K\n",
            "19  | model.line_cnn.convs.5.conv                                | Conv2d                  | 36.9 K\n",
            "20  | model.line_cnn.convs.5.relu                                | ReLU                    | 0     \n",
            "21  | model.line_cnn.convs.6                                     | ConvBlock               | 73.9 K\n",
            "22  | model.line_cnn.convs.6.conv                                | Conv2d                  | 73.9 K\n",
            "23  | model.line_cnn.convs.6.relu                                | ReLU                    | 0     \n",
            "24  | model.line_cnn.convs.7                                     | ConvBlock               | 147 K \n",
            "25  | model.line_cnn.convs.7.conv                                | Conv2d                  | 147 K \n",
            "26  | model.line_cnn.convs.7.relu                                | ReLU                    | 0     \n",
            "27  | model.line_cnn.convs.8                                     | ConvBlock               | 918 K \n",
            "28  | model.line_cnn.convs.8.conv                                | Conv2d                  | 918 K \n",
            "29  | model.line_cnn.convs.8.relu                                | ReLU                    | 0     \n",
            "30  | model.line_cnn.fc1                                         | Linear                  | 262 K \n",
            "31  | model.line_cnn.dropout                                     | Dropout                 | 0     \n",
            "32  | model.line_cnn.fc2                                         | Linear                  | 131 K \n",
            "33  | model.embedding                                            | Embedding               | 21.2 K\n",
            "34  | model.fc                                                   | Linear                  | 21.3 K\n",
            "35  | model.pos_encoder                                          | PositionalEncoding      | 0     \n",
            "36  | model.pos_encoder.dropout                                  | Dropout                 | 0     \n",
            "37  | model.transformer_decoder                                  | TransformerDecoder      | 2.6 M \n",
            "38  | model.transformer_decoder.layers                           | ModuleList              | 2.6 M \n",
            "39  | model.transformer_decoder.layers.0                         | TransformerDecoderLayer | 659 K \n",
            "40  | model.transformer_decoder.layers.0.self_attn               | MultiheadAttention      | 263 K \n",
            "41  | model.transformer_decoder.layers.0.self_attn.out_proj      | _LinearWithBias         | 65.8 K\n",
            "42  | model.transformer_decoder.layers.0.multihead_attn          | MultiheadAttention      | 263 K \n",
            "43  | model.transformer_decoder.layers.0.multihead_attn.out_proj | _LinearWithBias         | 65.8 K\n",
            "44  | model.transformer_decoder.layers.0.linear1                 | Linear                  | 65.8 K\n",
            "45  | model.transformer_decoder.layers.0.dropout                 | Dropout                 | 0     \n",
            "46  | model.transformer_decoder.layers.0.linear2                 | Linear                  | 65.8 K\n",
            "47  | model.transformer_decoder.layers.0.norm1                   | LayerNorm               | 512   \n",
            "48  | model.transformer_decoder.layers.0.norm2                   | LayerNorm               | 512   \n",
            "49  | model.transformer_decoder.layers.0.norm3                   | LayerNorm               | 512   \n",
            "50  | model.transformer_decoder.layers.0.dropout1                | Dropout                 | 0     \n",
            "51  | model.transformer_decoder.layers.0.dropout2                | Dropout                 | 0     \n",
            "52  | model.transformer_decoder.layers.0.dropout3                | Dropout                 | 0     \n",
            "53  | model.transformer_decoder.layers.1                         | TransformerDecoderLayer | 659 K \n",
            "54  | model.transformer_decoder.layers.1.self_attn               | MultiheadAttention      | 263 K \n",
            "55  | model.transformer_decoder.layers.1.self_attn.out_proj      | _LinearWithBias         | 65.8 K\n",
            "56  | model.transformer_decoder.layers.1.multihead_attn          | MultiheadAttention      | 263 K \n",
            "57  | model.transformer_decoder.layers.1.multihead_attn.out_proj | _LinearWithBias         | 65.8 K\n",
            "58  | model.transformer_decoder.layers.1.linear1                 | Linear                  | 65.8 K\n",
            "59  | model.transformer_decoder.layers.1.dropout                 | Dropout                 | 0     \n",
            "60  | model.transformer_decoder.layers.1.linear2                 | Linear                  | 65.8 K\n",
            "61  | model.transformer_decoder.layers.1.norm1                   | LayerNorm               | 512   \n",
            "62  | model.transformer_decoder.layers.1.norm2                   | LayerNorm               | 512   \n",
            "63  | model.transformer_decoder.layers.1.norm3                   | LayerNorm               | 512   \n",
            "64  | model.transformer_decoder.layers.1.dropout1                | Dropout                 | 0     \n",
            "65  | model.transformer_decoder.layers.1.dropout2                | Dropout                 | 0     \n",
            "66  | model.transformer_decoder.layers.1.dropout3                | Dropout                 | 0     \n",
            "67  | model.transformer_decoder.layers.2                         | TransformerDecoderLayer | 659 K \n",
            "68  | model.transformer_decoder.layers.2.self_attn               | MultiheadAttention      | 263 K \n",
            "69  | model.transformer_decoder.layers.2.self_attn.out_proj      | _LinearWithBias         | 65.8 K\n",
            "70  | model.transformer_decoder.layers.2.multihead_attn          | MultiheadAttention      | 263 K \n",
            "71  | model.transformer_decoder.layers.2.multihead_attn.out_proj | _LinearWithBias         | 65.8 K\n",
            "72  | model.transformer_decoder.layers.2.linear1                 | Linear                  | 65.8 K\n",
            "73  | model.transformer_decoder.layers.2.dropout                 | Dropout                 | 0     \n",
            "74  | model.transformer_decoder.layers.2.linear2                 | Linear                  | 65.8 K\n",
            "75  | model.transformer_decoder.layers.2.norm1                   | LayerNorm               | 512   \n",
            "76  | model.transformer_decoder.layers.2.norm2                   | LayerNorm               | 512   \n",
            "77  | model.transformer_decoder.layers.2.norm3                   | LayerNorm               | 512   \n",
            "78  | model.transformer_decoder.layers.2.dropout1                | Dropout                 | 0     \n",
            "79  | model.transformer_decoder.layers.2.dropout2                | Dropout                 | 0     \n",
            "80  | model.transformer_decoder.layers.2.dropout3                | Dropout                 | 0     \n",
            "81  | model.transformer_decoder.layers.3                         | TransformerDecoderLayer | 659 K \n",
            "82  | model.transformer_decoder.layers.3.self_attn               | MultiheadAttention      | 263 K \n",
            "83  | model.transformer_decoder.layers.3.self_attn.out_proj      | _LinearWithBias         | 65.8 K\n",
            "84  | model.transformer_decoder.layers.3.multihead_attn          | MultiheadAttention      | 263 K \n",
            "85  | model.transformer_decoder.layers.3.multihead_attn.out_proj | _LinearWithBias         | 65.8 K\n",
            "86  | model.transformer_decoder.layers.3.linear1                 | Linear                  | 65.8 K\n",
            "87  | model.transformer_decoder.layers.3.dropout                 | Dropout                 | 0     \n",
            "88  | model.transformer_decoder.layers.3.linear2                 | Linear                  | 65.8 K\n",
            "89  | model.transformer_decoder.layers.3.norm1                   | LayerNorm               | 512   \n",
            "90  | model.transformer_decoder.layers.3.norm2                   | LayerNorm               | 512   \n",
            "91  | model.transformer_decoder.layers.3.norm3                   | LayerNorm               | 512   \n",
            "92  | model.transformer_decoder.layers.3.dropout1                | Dropout                 | 0     \n",
            "93  | model.transformer_decoder.layers.3.dropout2                | Dropout                 | 0     \n",
            "94  | model.transformer_decoder.layers.3.dropout3                | Dropout                 | 0     \n",
            "95  | train_acc                                                  | Accuracy                | 0     \n",
            "96  | val_acc                                                    | Accuracy                | 0     \n",
            "97  | test_acc                                                   | Accuracy                | 0     \n",
            "98  | loss_fn                                                    | CrossEntropyLoss        | 0     \n",
            "99  | val_cer                                                    | CharacterErrorRate      | 0     \n",
            "100 | test_cer                                                   | CharacterErrorRate      | 0     \n",
            "---------------------------------------------------------------------------------------------------------\n",
            "4.3 M     Trainable params\n",
            "0         Non-trainable params\n",
            "4.3 M     Total params\n",
            "Epoch 0:  83% 79/95 [01:59<00:24,  1.51s/it, loss=2.85, v_num=yf2o, val_loss=4.8, val_cer=0.998]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 0:  85% 81/95 [02:06<00:21,  1.56s/it, loss=2.85, v_num=yf2o, val_loss=4.8, val_cer=0.998]\n",
            "Validating:  12% 2/16 [00:13<01:39,  7.08s/it]\u001b[A\n",
            "Epoch 0:  87% 83/95 [02:19<00:20,  1.68s/it, loss=2.85, v_num=yf2o, val_loss=4.8, val_cer=0.998]\n",
            "Epoch 0:  88% 84/95 [02:25<00:19,  1.73s/it, loss=2.85, v_num=yf2o, val_loss=4.8, val_cer=0.998]\n",
            "Epoch 0:  89% 85/95 [02:31<00:17,  1.78s/it, loss=2.85, v_num=yf2o, val_loss=4.8, val_cer=0.998]\n",
            "Epoch 0:  91% 86/95 [02:37<00:16,  1.84s/it, loss=2.85, v_num=yf2o, val_loss=4.8, val_cer=0.998]\n",
            "Epoch 0:  92% 87/95 [02:44<00:15,  1.89s/it, loss=2.85, v_num=yf2o, val_loss=4.8, val_cer=0.998]\n",
            "Epoch 0:  93% 88/95 [02:50<00:13,  1.94s/it, loss=2.85, v_num=yf2o, val_loss=4.8, val_cer=0.998]\n",
            "Epoch 0:  94% 89/95 [02:56<00:11,  1.99s/it, loss=2.85, v_num=yf2o, val_loss=4.8, val_cer=0.998]\n",
            "Epoch 0:  95% 90/95 [03:03<00:10,  2.03s/it, loss=2.85, v_num=yf2o, val_loss=4.8, val_cer=0.998]\n",
            "Epoch 0:  96% 91/95 [03:09<00:08,  2.08s/it, loss=2.85, v_num=yf2o, val_loss=4.8, val_cer=0.998]\n",
            "Epoch 0:  97% 92/95 [03:15<00:06,  2.13s/it, loss=2.85, v_num=yf2o, val_loss=4.8, val_cer=0.998]\n",
            "Epoch 0:  98% 93/95 [03:21<00:04,  2.17s/it, loss=2.85, v_num=yf2o, val_loss=4.8, val_cer=0.998]\n",
            "Epoch 0:  99% 94/95 [03:28<00:02,  2.21s/it, loss=2.85, v_num=yf2o, val_loss=4.8, val_cer=0.998]\n",
            "Epoch 0: 100% 95/95 [03:34<00:00,  2.26s/it, loss=2.85, v_num=yf2o, val_loss=4.8, val_cer=0.998]\n",
            "Epoch 0: 100% 95/95 [03:39<00:00,  2.31s/it, loss=2.85, v_num=yf2o, val_loss=2.64, val_cer=0.89]\n",
            "Epoch 1:  83% 79/95 [01:57<00:23,  1.49s/it, loss=2.47, v_num=yf2o, val_loss=2.64, val_cer=0.89]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 1:  85% 81/95 [02:05<00:21,  1.55s/it, loss=2.47, v_num=yf2o, val_loss=2.64, val_cer=0.89]\n",
            "Validating:  12% 2/16 [00:13<01:39,  7.11s/it]\u001b[A\n",
            "Epoch 1:  87% 83/95 [02:17<00:19,  1.66s/it, loss=2.47, v_num=yf2o, val_loss=2.64, val_cer=0.89]\n",
            "Epoch 1:  88% 84/95 [02:24<00:18,  1.72s/it, loss=2.47, v_num=yf2o, val_loss=2.64, val_cer=0.89]\n",
            "Epoch 1:  89% 85/95 [02:30<00:17,  1.77s/it, loss=2.47, v_num=yf2o, val_loss=2.64, val_cer=0.89]\n",
            "Epoch 1:  91% 86/95 [02:36<00:16,  1.82s/it, loss=2.47, v_num=yf2o, val_loss=2.64, val_cer=0.89]\n",
            "Epoch 1:  92% 87/95 [02:43<00:14,  1.87s/it, loss=2.47, v_num=yf2o, val_loss=2.64, val_cer=0.89]\n",
            "Epoch 1:  93% 88/95 [02:49<00:13,  1.92s/it, loss=2.47, v_num=yf2o, val_loss=2.64, val_cer=0.89]\n",
            "Epoch 1:  94% 89/95 [02:55<00:11,  1.97s/it, loss=2.47, v_num=yf2o, val_loss=2.64, val_cer=0.89]\n",
            "Epoch 1:  95% 90/95 [03:01<00:10,  2.02s/it, loss=2.47, v_num=yf2o, val_loss=2.64, val_cer=0.89]\n",
            "Epoch 1:  96% 91/95 [03:08<00:08,  2.07s/it, loss=2.47, v_num=yf2o, val_loss=2.64, val_cer=0.89]\n",
            "Epoch 1:  97% 92/95 [03:14<00:06,  2.11s/it, loss=2.47, v_num=yf2o, val_loss=2.64, val_cer=0.89]\n",
            "Epoch 1:  98% 93/95 [03:20<00:04,  2.16s/it, loss=2.47, v_num=yf2o, val_loss=2.64, val_cer=0.89]\n",
            "Epoch 1:  99% 94/95 [03:27<00:02,  2.20s/it, loss=2.47, v_num=yf2o, val_loss=2.64, val_cer=0.89]\n",
            "Epoch 1: 100% 95/95 [03:33<00:00,  2.25s/it, loss=2.47, v_num=yf2o, val_loss=2.64, val_cer=0.89]\n",
            "Epoch 1: 100% 95/95 [03:37<00:00,  2.29s/it, loss=2.47, v_num=yf2o, val_loss=2.36, val_cer=0.811]\n",
            "                                               \u001b[A/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Trying to log at a previous step. Use `commit=False` when logging metrics manually.\n",
            "  warnings.warn(*args, **kwargs)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 157 < 165; dropping {'val_loss': 2.362567663192749, 'val_cer': 0.811098039150238, 'epoch': 1}.\n",
            "Epoch 2:  83% 79/95 [01:57<00:23,  1.49s/it, loss=2.35, v_num=yf2o, val_loss=2.36, val_cer=0.811]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 2:  85% 81/95 [02:05<00:21,  1.54s/it, loss=2.35, v_num=yf2o, val_loss=2.36, val_cer=0.811]\n",
            "Validating:  12% 2/16 [00:13<01:39,  7.10s/it]\u001b[A\n",
            "Epoch 2:  87% 83/95 [02:17<00:19,  1.66s/it, loss=2.35, v_num=yf2o, val_loss=2.36, val_cer=0.811]\n",
            "Epoch 2:  88% 84/95 [02:23<00:18,  1.71s/it, loss=2.35, v_num=yf2o, val_loss=2.36, val_cer=0.811]\n",
            "Epoch 2:  89% 85/95 [02:30<00:17,  1.77s/it, loss=2.35, v_num=yf2o, val_loss=2.36, val_cer=0.811]\n",
            "Epoch 2:  91% 86/95 [02:36<00:16,  1.82s/it, loss=2.35, v_num=yf2o, val_loss=2.36, val_cer=0.811]\n",
            "Epoch 2:  92% 87/95 [02:42<00:14,  1.87s/it, loss=2.35, v_num=yf2o, val_loss=2.36, val_cer=0.811]\n",
            "Epoch 2:  93% 88/95 [02:49<00:13,  1.92s/it, loss=2.35, v_num=yf2o, val_loss=2.36, val_cer=0.811]\n",
            "Epoch 2:  94% 89/95 [02:55<00:11,  1.97s/it, loss=2.35, v_num=yf2o, val_loss=2.36, val_cer=0.811]\n",
            "Epoch 2:  95% 90/95 [03:01<00:10,  2.02s/it, loss=2.35, v_num=yf2o, val_loss=2.36, val_cer=0.811]\n",
            "Epoch 2:  96% 91/95 [03:07<00:08,  2.06s/it, loss=2.35, v_num=yf2o, val_loss=2.36, val_cer=0.811]\n",
            "Epoch 2:  97% 92/95 [03:14<00:06,  2.11s/it, loss=2.35, v_num=yf2o, val_loss=2.36, val_cer=0.811]\n",
            "Epoch 2:  98% 93/95 [03:20<00:04,  2.16s/it, loss=2.35, v_num=yf2o, val_loss=2.36, val_cer=0.811]\n",
            "Epoch 2:  99% 94/95 [03:26<00:02,  2.20s/it, loss=2.35, v_num=yf2o, val_loss=2.36, val_cer=0.811]\n",
            "Epoch 2: 100% 95/95 [03:33<00:00,  2.24s/it, loss=2.35, v_num=yf2o, val_loss=2.36, val_cer=0.811]\n",
            "Epoch 2: 100% 95/95 [03:37<00:00,  2.29s/it, loss=2.35, v_num=yf2o, val_loss=2.24, val_cer=0.821]\n",
            "Epoch 3:  83% 79/95 [01:57<00:23,  1.49s/it, loss=2.28, v_num=yf2o, val_loss=2.24, val_cer=0.821]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 3:  85% 81/95 [02:05<00:21,  1.55s/it, loss=2.28, v_num=yf2o, val_loss=2.24, val_cer=0.821]\n",
            "Validating:  12% 2/16 [00:13<01:40,  7.18s/it]\u001b[A\n",
            "Epoch 3:  87% 83/95 [02:17<00:19,  1.66s/it, loss=2.28, v_num=yf2o, val_loss=2.24, val_cer=0.821]\n",
            "Epoch 3:  88% 84/95 [02:24<00:18,  1.72s/it, loss=2.28, v_num=yf2o, val_loss=2.24, val_cer=0.821]\n",
            "Epoch 3:  89% 85/95 [02:30<00:17,  1.77s/it, loss=2.28, v_num=yf2o, val_loss=2.24, val_cer=0.821]\n",
            "Epoch 3:  91% 86/95 [02:36<00:16,  1.82s/it, loss=2.28, v_num=yf2o, val_loss=2.24, val_cer=0.821]\n",
            "Epoch 3:  92% 87/95 [02:43<00:14,  1.87s/it, loss=2.28, v_num=yf2o, val_loss=2.24, val_cer=0.821]\n",
            "Epoch 3:  93% 88/95 [02:49<00:13,  1.92s/it, loss=2.28, v_num=yf2o, val_loss=2.24, val_cer=0.821]\n",
            "Epoch 3:  94% 89/95 [02:55<00:11,  1.97s/it, loss=2.28, v_num=yf2o, val_loss=2.24, val_cer=0.821]\n",
            "Epoch 3:  95% 90/95 [03:01<00:10,  2.02s/it, loss=2.28, v_num=yf2o, val_loss=2.24, val_cer=0.821]\n",
            "Epoch 3:  96% 91/95 [03:08<00:08,  2.07s/it, loss=2.28, v_num=yf2o, val_loss=2.24, val_cer=0.821]\n",
            "Epoch 3:  97% 92/95 [03:14<00:06,  2.11s/it, loss=2.28, v_num=yf2o, val_loss=2.24, val_cer=0.821]\n",
            "Epoch 3:  98% 93/95 [03:20<00:04,  2.16s/it, loss=2.28, v_num=yf2o, val_loss=2.24, val_cer=0.821]\n",
            "Epoch 3:  99% 94/95 [03:27<00:02,  2.20s/it, loss=2.28, v_num=yf2o, val_loss=2.24, val_cer=0.821]\n",
            "Epoch 3: 100% 95/95 [03:33<00:00,  2.25s/it, loss=2.28, v_num=yf2o, val_loss=2.24, val_cer=0.821]\n",
            "Epoch 3: 100% 95/95 [03:38<00:00,  2.30s/it, loss=2.28, v_num=yf2o, val_loss=2.17, val_cer=0.821]\n",
            "Epoch 4:  83% 79/95 [01:57<00:23,  1.49s/it, loss=2.24, v_num=yf2o, val_loss=2.17, val_cer=0.821]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 4:  85% 81/95 [02:05<00:21,  1.54s/it, loss=2.24, v_num=yf2o, val_loss=2.17, val_cer=0.821]\n",
            "Epoch 4:  86% 82/95 [02:17<00:21,  1.67s/it, loss=2.24, v_num=yf2o, val_loss=2.17, val_cer=0.821]\n",
            "Epoch 4:  87% 83/95 [02:17<00:19,  1.66s/it, loss=2.24, v_num=yf2o, val_loss=2.17, val_cer=0.821]\n",
            "Epoch 4:  88% 84/95 [02:23<00:18,  1.71s/it, loss=2.24, v_num=yf2o, val_loss=2.17, val_cer=0.821]\n",
            "Epoch 4:  89% 85/95 [02:30<00:17,  1.77s/it, loss=2.24, v_num=yf2o, val_loss=2.17, val_cer=0.821]\n",
            "Epoch 4:  91% 86/95 [02:36<00:16,  1.82s/it, loss=2.24, v_num=yf2o, val_loss=2.17, val_cer=0.821]\n",
            "Epoch 4:  92% 87/95 [02:42<00:14,  1.87s/it, loss=2.24, v_num=yf2o, val_loss=2.17, val_cer=0.821]\n",
            "Epoch 4:  93% 88/95 [02:49<00:13,  1.92s/it, loss=2.24, v_num=yf2o, val_loss=2.17, val_cer=0.821]\n",
            "Epoch 4:  94% 89/95 [02:55<00:11,  1.97s/it, loss=2.24, v_num=yf2o, val_loss=2.17, val_cer=0.821]\n",
            "Epoch 4:  95% 90/95 [03:01<00:10,  2.02s/it, loss=2.24, v_num=yf2o, val_loss=2.17, val_cer=0.821]\n",
            "Epoch 4:  96% 91/95 [03:08<00:08,  2.07s/it, loss=2.24, v_num=yf2o, val_loss=2.17, val_cer=0.821]\n",
            "Epoch 4:  97% 92/95 [03:14<00:06,  2.11s/it, loss=2.24, v_num=yf2o, val_loss=2.17, val_cer=0.821]\n",
            "Epoch 4:  98% 93/95 [03:20<00:04,  2.16s/it, loss=2.24, v_num=yf2o, val_loss=2.17, val_cer=0.821]\n",
            "Epoch 4:  99% 94/95 [03:26<00:02,  2.20s/it, loss=2.24, v_num=yf2o, val_loss=2.17, val_cer=0.821]\n",
            "Epoch 4: 100% 95/95 [03:33<00:00,  2.24s/it, loss=2.24, v_num=yf2o, val_loss=2.17, val_cer=0.821]\n",
            "Epoch 4: 100% 95/95 [03:37<00:00,  2.29s/it, loss=2.24, v_num=yf2o, val_loss=2.12, val_cer=0.796]\n",
            "Epoch 5:  83% 79/95 [01:57<00:23,  1.49s/it, loss=2.2, v_num=yf2o, val_loss=2.12, val_cer=0.796]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 5:  85% 81/95 [02:05<00:21,  1.55s/it, loss=2.2, v_num=yf2o, val_loss=2.12, val_cer=0.796]\n",
            "Validating:  12% 2/16 [00:13<01:39,  7.11s/it]\u001b[A\n",
            "Epoch 5:  87% 83/95 [02:17<00:19,  1.66s/it, loss=2.2, v_num=yf2o, val_loss=2.12, val_cer=0.796]\n",
            "Epoch 5:  88% 84/95 [02:24<00:18,  1.71s/it, loss=2.2, v_num=yf2o, val_loss=2.12, val_cer=0.796]\n",
            "Epoch 5:  89% 85/95 [02:30<00:17,  1.77s/it, loss=2.2, v_num=yf2o, val_loss=2.12, val_cer=0.796]\n",
            "Epoch 5:  91% 86/95 [02:36<00:16,  1.82s/it, loss=2.2, v_num=yf2o, val_loss=2.12, val_cer=0.796]\n",
            "Epoch 5:  92% 87/95 [02:42<00:14,  1.87s/it, loss=2.2, v_num=yf2o, val_loss=2.12, val_cer=0.796]\n",
            "Epoch 5:  93% 88/95 [02:49<00:13,  1.92s/it, loss=2.2, v_num=yf2o, val_loss=2.12, val_cer=0.796]\n",
            "Epoch 5:  94% 89/95 [02:55<00:11,  1.97s/it, loss=2.2, v_num=yf2o, val_loss=2.12, val_cer=0.796]\n",
            "Epoch 5:  95% 90/95 [03:01<00:10,  2.02s/it, loss=2.2, v_num=yf2o, val_loss=2.12, val_cer=0.796]\n",
            "Epoch 5:  96% 91/95 [03:08<00:08,  2.07s/it, loss=2.2, v_num=yf2o, val_loss=2.12, val_cer=0.796]\n",
            "Epoch 5:  97% 92/95 [03:14<00:06,  2.11s/it, loss=2.2, v_num=yf2o, val_loss=2.12, val_cer=0.796]\n",
            "Epoch 5:  98% 93/95 [03:20<00:04,  2.16s/it, loss=2.2, v_num=yf2o, val_loss=2.12, val_cer=0.796]\n",
            "Epoch 5:  99% 94/95 [03:27<00:02,  2.20s/it, loss=2.2, v_num=yf2o, val_loss=2.12, val_cer=0.796]\n",
            "Epoch 5: 100% 95/95 [03:33<00:00,  2.25s/it, loss=2.2, v_num=yf2o, val_loss=2.12, val_cer=0.796]\n",
            "Epoch 5: 100% 95/95 [03:37<00:00,  2.29s/it, loss=2.2, v_num=yf2o, val_loss=2.09, val_cer=0.821]\n",
            "Epoch 6:  83% 79/95 [01:57<00:23,  1.49s/it, loss=2.15, v_num=yf2o, val_loss=2.09, val_cer=0.821]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 6:  85% 81/95 [02:05<00:21,  1.55s/it, loss=2.15, v_num=yf2o, val_loss=2.09, val_cer=0.821]\n",
            "Validating:  12% 2/16 [00:13<01:40,  7.17s/it]\u001b[A\n",
            "Epoch 6:  87% 83/95 [02:17<00:19,  1.66s/it, loss=2.15, v_num=yf2o, val_loss=2.09, val_cer=0.821]\n",
            "Epoch 6:  88% 84/95 [02:24<00:18,  1.72s/it, loss=2.15, v_num=yf2o, val_loss=2.09, val_cer=0.821]\n",
            "Epoch 6:  89% 85/95 [02:30<00:17,  1.77s/it, loss=2.15, v_num=yf2o, val_loss=2.09, val_cer=0.821]\n",
            "Epoch 6:  91% 86/95 [02:36<00:16,  1.82s/it, loss=2.15, v_num=yf2o, val_loss=2.09, val_cer=0.821]\n",
            "Epoch 6:  92% 87/95 [02:43<00:15,  1.88s/it, loss=2.15, v_num=yf2o, val_loss=2.09, val_cer=0.821]\n",
            "Epoch 6:  93% 88/95 [02:49<00:13,  1.93s/it, loss=2.15, v_num=yf2o, val_loss=2.09, val_cer=0.821]\n",
            "Epoch 6:  94% 89/95 [02:55<00:11,  1.97s/it, loss=2.15, v_num=yf2o, val_loss=2.09, val_cer=0.821]\n",
            "Epoch 6:  95% 90/95 [03:02<00:10,  2.02s/it, loss=2.15, v_num=yf2o, val_loss=2.09, val_cer=0.821]\n",
            "Epoch 6:  96% 91/95 [03:08<00:08,  2.07s/it, loss=2.15, v_num=yf2o, val_loss=2.09, val_cer=0.821]\n",
            "Epoch 6:  97% 92/95 [03:14<00:06,  2.12s/it, loss=2.15, v_num=yf2o, val_loss=2.09, val_cer=0.821]\n",
            "Epoch 6:  98% 93/95 [03:20<00:04,  2.16s/it, loss=2.15, v_num=yf2o, val_loss=2.09, val_cer=0.821]\n",
            "Epoch 6:  99% 94/95 [03:27<00:02,  2.20s/it, loss=2.15, v_num=yf2o, val_loss=2.09, val_cer=0.821]\n",
            "Epoch 6: 100% 95/95 [03:33<00:00,  2.25s/it, loss=2.15, v_num=yf2o, val_loss=2.09, val_cer=0.821]\n",
            "Epoch 6: 100% 95/95 [03:38<00:00,  2.30s/it, loss=2.15, v_num=yf2o, val_loss=2.07, val_cer=0.792]\n",
            "                                               \u001b[A\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 552 < 565; dropping {'val_loss': 2.0706064701080322, 'val_cer': 0.7921993732452393, 'epoch': 6}.\n",
            "Epoch 7:  83% 79/95 [01:57<00:23,  1.49s/it, loss=2.14, v_num=yf2o, val_loss=2.07, val_cer=0.792]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 7:  85% 81/95 [02:05<00:21,  1.55s/it, loss=2.14, v_num=yf2o, val_loss=2.07, val_cer=0.792]\n",
            "Validating:  12% 2/16 [00:13<01:38,  7.06s/it]\u001b[A\n",
            "Epoch 7:  87% 83/95 [02:17<00:19,  1.66s/it, loss=2.14, v_num=yf2o, val_loss=2.07, val_cer=0.792]\n",
            "Epoch 7:  88% 84/95 [02:23<00:18,  1.71s/it, loss=2.14, v_num=yf2o, val_loss=2.07, val_cer=0.792]\n",
            "Epoch 7:  89% 85/95 [02:30<00:17,  1.77s/it, loss=2.14, v_num=yf2o, val_loss=2.07, val_cer=0.792]\n",
            "Epoch 7:  91% 86/95 [02:36<00:16,  1.82s/it, loss=2.14, v_num=yf2o, val_loss=2.07, val_cer=0.792]\n",
            "Epoch 7:  92% 87/95 [02:42<00:14,  1.87s/it, loss=2.14, v_num=yf2o, val_loss=2.07, val_cer=0.792]\n",
            "Epoch 7:  93% 88/95 [02:49<00:13,  1.92s/it, loss=2.14, v_num=yf2o, val_loss=2.07, val_cer=0.792]\n",
            "Epoch 7:  94% 89/95 [02:55<00:11,  1.97s/it, loss=2.14, v_num=yf2o, val_loss=2.07, val_cer=0.792]\n",
            "Epoch 7:  95% 90/95 [03:01<00:10,  2.02s/it, loss=2.14, v_num=yf2o, val_loss=2.07, val_cer=0.792]\n",
            "Epoch 7:  96% 91/95 [03:08<00:08,  2.07s/it, loss=2.14, v_num=yf2o, val_loss=2.07, val_cer=0.792]\n",
            "Epoch 7:  97% 92/95 [03:14<00:06,  2.11s/it, loss=2.14, v_num=yf2o, val_loss=2.07, val_cer=0.792]\n",
            "Epoch 7:  98% 93/95 [03:20<00:04,  2.16s/it, loss=2.14, v_num=yf2o, val_loss=2.07, val_cer=0.792]\n",
            "Epoch 7:  99% 94/95 [03:26<00:02,  2.20s/it, loss=2.14, v_num=yf2o, val_loss=2.07, val_cer=0.792]\n",
            "Epoch 7: 100% 95/95 [03:33<00:00,  2.24s/it, loss=2.14, v_num=yf2o, val_loss=2.07, val_cer=0.792]\n",
            "Epoch 7: 100% 95/95 [03:37<00:00,  2.29s/it, loss=2.14, v_num=yf2o, val_loss=2.04, val_cer=0.797]\n",
            "Epoch 8:  83% 79/95 [01:57<00:23,  1.49s/it, loss=2.13, v_num=yf2o, val_loss=2.04, val_cer=0.797]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 8:  85% 81/95 [02:05<00:21,  1.55s/it, loss=2.13, v_num=yf2o, val_loss=2.04, val_cer=0.797]\n",
            "Epoch 8:  86% 82/95 [02:15<00:21,  1.65s/it, loss=2.13, v_num=yf2o, val_loss=2.04, val_cer=0.797]\n",
            "Epoch 8:  87% 83/95 [02:17<00:19,  1.66s/it, loss=2.13, v_num=yf2o, val_loss=2.04, val_cer=0.797]\n",
            "Epoch 8:  88% 84/95 [02:24<00:18,  1.72s/it, loss=2.13, v_num=yf2o, val_loss=2.04, val_cer=0.797]\n",
            "Epoch 8:  89% 85/95 [02:30<00:17,  1.77s/it, loss=2.13, v_num=yf2o, val_loss=2.04, val_cer=0.797]\n",
            "Epoch 8:  91% 86/95 [02:36<00:16,  1.82s/it, loss=2.13, v_num=yf2o, val_loss=2.04, val_cer=0.797]\n",
            "Epoch 8:  92% 87/95 [02:43<00:14,  1.87s/it, loss=2.13, v_num=yf2o, val_loss=2.04, val_cer=0.797]\n",
            "Epoch 8:  93% 88/95 [02:49<00:13,  1.92s/it, loss=2.13, v_num=yf2o, val_loss=2.04, val_cer=0.797]\n",
            "Epoch 8:  94% 89/95 [02:55<00:11,  1.97s/it, loss=2.13, v_num=yf2o, val_loss=2.04, val_cer=0.797]\n",
            "Epoch 8:  95% 90/95 [03:02<00:10,  2.02s/it, loss=2.13, v_num=yf2o, val_loss=2.04, val_cer=0.797]\n",
            "Epoch 8:  96% 91/95 [03:08<00:08,  2.07s/it, loss=2.13, v_num=yf2o, val_loss=2.04, val_cer=0.797]\n",
            "Epoch 8:  97% 92/95 [03:14<00:06,  2.12s/it, loss=2.13, v_num=yf2o, val_loss=2.04, val_cer=0.797]\n",
            "Epoch 8:  98% 93/95 [03:21<00:04,  2.16s/it, loss=2.13, v_num=yf2o, val_loss=2.04, val_cer=0.797]\n",
            "Epoch 8:  99% 94/95 [03:27<00:02,  2.21s/it, loss=2.13, v_num=yf2o, val_loss=2.04, val_cer=0.797]\n",
            "Epoch 8: 100% 95/95 [03:33<00:00,  2.25s/it, loss=2.13, v_num=yf2o, val_loss=2.04, val_cer=0.797]\n",
            "Epoch 8: 100% 95/95 [03:38<00:00,  2.30s/it, loss=2.13, v_num=yf2o, val_loss=2.02, val_cer=0.797]\n",
            "                                               \u001b[A\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 710 < 715; dropping {'val_loss': 2.0208804607391357, 'val_cer': 0.7970757484436035, 'epoch': 8}.\n",
            "Epoch 9:  83% 79/95 [01:57<00:23,  1.49s/it, loss=2.1, v_num=yf2o, val_loss=2.02, val_cer=0.797]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 9:  85% 81/95 [02:05<00:21,  1.55s/it, loss=2.1, v_num=yf2o, val_loss=2.02, val_cer=0.797]\n",
            "Epoch 9:  86% 82/95 [02:17<00:21,  1.67s/it, loss=2.1, v_num=yf2o, val_loss=2.02, val_cer=0.797]\n",
            "Epoch 9:  87% 83/95 [02:17<00:19,  1.66s/it, loss=2.1, v_num=yf2o, val_loss=2.02, val_cer=0.797]\n",
            "Epoch 9:  88% 84/95 [02:24<00:18,  1.72s/it, loss=2.1, v_num=yf2o, val_loss=2.02, val_cer=0.797]\n",
            "Epoch 9:  89% 85/95 [02:30<00:17,  1.77s/it, loss=2.1, v_num=yf2o, val_loss=2.02, val_cer=0.797]\n",
            "Epoch 9:  91% 86/95 [02:36<00:16,  1.82s/it, loss=2.1, v_num=yf2o, val_loss=2.02, val_cer=0.797]\n",
            "Epoch 9:  92% 87/95 [02:43<00:15,  1.88s/it, loss=2.1, v_num=yf2o, val_loss=2.02, val_cer=0.797]\n",
            "Epoch 9:  93% 88/95 [02:49<00:13,  1.93s/it, loss=2.1, v_num=yf2o, val_loss=2.02, val_cer=0.797]\n",
            "Epoch 9:  94% 89/95 [02:55<00:11,  1.98s/it, loss=2.1, v_num=yf2o, val_loss=2.02, val_cer=0.797]\n",
            "Epoch 9:  95% 90/95 [03:02<00:10,  2.02s/it, loss=2.1, v_num=yf2o, val_loss=2.02, val_cer=0.797]\n",
            "Epoch 9:  96% 91/95 [03:08<00:08,  2.07s/it, loss=2.1, v_num=yf2o, val_loss=2.02, val_cer=0.797]\n",
            "Epoch 9:  97% 92/95 [03:14<00:06,  2.12s/it, loss=2.1, v_num=yf2o, val_loss=2.02, val_cer=0.797]\n",
            "Epoch 9:  98% 93/95 [03:21<00:04,  2.16s/it, loss=2.1, v_num=yf2o, val_loss=2.02, val_cer=0.797]\n",
            "Epoch 9:  99% 94/95 [03:27<00:02,  2.21s/it, loss=2.1, v_num=yf2o, val_loss=2.02, val_cer=0.797]\n",
            "Epoch 9: 100% 95/95 [03:33<00:00,  2.25s/it, loss=2.1, v_num=yf2o, val_loss=2.02, val_cer=0.797]\n",
            "Epoch 9: 100% 95/95 [03:38<00:00,  2.30s/it, loss=2.1, v_num=yf2o, val_loss=2.02, val_cer=0.792]\n",
            "Epoch 9: 100% 95/95 [03:38<00:00,  2.30s/it, loss=2.1, v_num=yf2o, val_loss=2.02, val_cer=0.792]\n",
            "EMNISTLines2 loading data from HDF5...\n",
            "Testing: 100% 16/16 [01:31<00:00,  5.74s/it]\n",
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'test_cer': tensor(0.7913, device='cuda:0')}\n",
            "--------------------------------------------------------------------------------\n",
            "Best model saved at: training/logs/fsdl-text-recognizer-2021-labs/206iyf2o/checkpoints/epoch=009-val_loss=2.015-val_cer=0.792.ckpt\n",
            "Best model also uploaded to W&B\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 255\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/fsdl-text-recognizer-2021-labs/lab5/wandb/run-20210623_140927-206iyf2o/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/fsdl-text-recognizer-2021-labs/lab5/wandb/run-20210623_140927-206iyf2o/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                _runtime 2442\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                              _timestamp 1624459809\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                   _step 1579\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                              train_loss 2.12357\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                   epoch 9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                val_loss 2.01516\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                 val_cer 0.79199\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                test_cer 0.79127\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     _runtime ▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇██████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   _timestamp ▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇██████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _step ▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇███████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   train_loss █▄▃▃▃▂▂▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        epoch ▁▁▂▂▃▃▃▃▃▄▄▅▅▅▆▆▆▆▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     val_loss █▄▃▂▂▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val_cer █▃▃▁▃▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     test_cer ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 179 media file(s), 0 artifact file(s) and 1 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msweet-valley-3\u001b[0m: \u001b[34mhttps://wandb.ai/linxiao/fsdl-text-recognizer-2021-labs/runs/206iyf2o\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}